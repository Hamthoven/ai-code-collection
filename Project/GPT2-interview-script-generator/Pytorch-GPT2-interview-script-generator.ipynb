{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets\n",
    "! pip install transformers[torch]\n",
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"glavin001/startup-interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"instruction\"]\n",
    "    targets = examples[\"start\"]\n",
    "    return {\"inputs\": inputs, \"targets\": targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set block size based on your dataset and memory constraints\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and collate the dataset\n",
    "def collate_function(examples):\n",
    "    return tokenizer(examples[\"inputs\"], max_length=block_size, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_datasets = tokenized_datasets.map(collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set block size for training\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "tokenized_datasets.set_format(columns=[\"input_ids\", \"attention_mask\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_total_limit=2,\n",
    "    save_steps=10_000,\n",
    "    learning_rate=0.01,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"path/to/saved/model\")\n",
    "tokenizer.save_pretrained(\"path/to/saved/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"path/to/saved/model\")\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"path/to/saved/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an example\n",
    "prompt = \"Can you please introduce yourself and tell us about your role at Y Combinator?\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "output = fine_tuned_model.generate(input_ids, max_length=150, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, pad_token_id=fine_tuned_model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Example:\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Example:    \n",
    "prompt: Can you please introduce yourself and tell us about your role at Y Combinator?    \n",
    "model: Can you please introduce yourself and tell us about your role at Y Combinator? startup??ator and? the? in?bin? idea? and the a? team?C? Com? of the the startup and a a startup in the it?,? their? product? a the of a it and startup to? you the you? itator in a of your the company? is the how? users? initial? them? are the startups? market? your a you a how and your startup of startupbin and how in your your itbin in it in startupator to the product and of how a your how of it to a Y? success? early? what the Y and company and you startup on? to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an example\n",
    "prompt = \"Do you hae any experiences related with the SW Engineer?\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model.generate(input_ids, max_length=150, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, pad_token_id=fine_tuned_model.config.pad_token_id)\n",
    "\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Example:\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Example:    \n",
    "prompt: Do you hae any experiences related with the SW Engineer?    \n",
    "model: Do you hae any experiences related with the SW Engineer? startup?? and the? the the startup and?bin?ator? team? in? a the a? idea? it? Com?C?,? of? are the how? ideas? market? them? is the success? their? to a a startup in the itator and a it and your startup to the company? users?'s? fit? that? industry? on? initial? product? or the your the have? growth? first? venture? for? from? you a how and howator in a success and itbin and startup of the you the Y? stage? new? early?-? company and some? business? funding? what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an example\n",
    "prompt = \"Do you hae any experiences related with the SW Engineer? one answer, 200 words, job description please.\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model.generate(input_ids, max_length=150, num_beams=5, no_repeat_ngram_size=1, top_k=2, top_p=0.2, pad_token_id=fine_tuned_model.config.pad_token_id)\n",
    "\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Example:\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Example:    \n",
    "prompt: Do you hae any experiences related with the SW Engineer? one answer, 200 words, job description please.    \n",
    "model: Do you hae any experiences related with the SW Engineer? one answer, 200 words, job description please. to startup in and a of it for on your have- that idea or productbinator are how from's their ideas is what successIs when Y at Com team aboutC stage funding do growth together as be company industry who after startups factor during early people fit)? KP ( does new business MVP impact up them time models\" can some world without has this into by hard market venture between so make its if first project over find interview rate before users founders rounds through were should starting work they model using fail 3 data among help came did other my towards strategies tech members school face will building goals used advice factors out come performancetech co takeing companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an example\n",
    "prompt = \"Do you hae any experiences related with the SW Engineer? one answer, 200 words, job description please.\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100, num_beams=2, no_repeat_ngram_size=1, top_k=1, top_p=0.6, pad_token_id=fine_tuned_model.config.pad_token_id)\n",
    "\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Example:\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Example:    \n",
    "prompt: Do you hae any experiences related with the SW Engineer? one answer, 200 words, job description please.    \n",
    "model: Do you hae any experiences related with the SW Engineer? one answer, 200 words, job description please. to startup and of a in it for on your have- that or productbinator are how idea's their company from at startups is successIs when what Y ideasC team about Com industry stage funding as do growth who together after be business people fit factor during early KP ( up them MVP impact\" time market)? between new world without does can some venture make has this into by goals models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an example\n",
    "prompt = \"Do you have any experiences related to SW Engineer in startup company?\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100, num_beams=8, no_repeat_ngram_size=1, top_k=1, top_p=0.95, pad_token_id=fine_tuned_model.config.pad_token_id)\n",
    "\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Example:\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Example:    \n",
    "prompt: Do you have any experiences related to SW Engineer in startup company?    \n",
    "model: Do you have any experiences related to SW Engineer in startup company? the success and a of your it Ybinator for, idea or team on their are how- thatC ideas's product funding from with ventureIs when Com industry at is what be growth stage market fit together people them)? about first who world KP do business new time so MVP after early models during startups were starting future as this ( has hard users rounds up project factor impact make founders tech fail without can some work school data before members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an example\n",
    "prompt = \"What strategies can be used to encourage users to provide more detailed and comprehensive answers during interviews?\"\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output = fine_tuned_model.generate(input_ids, max_length=100, num_beams=8, no_repeat_ngram_size=1, top_k=3, top_p=0.9, pad_token_id=fine_tuned_model.config.pad_token_id)\n",
    "\n",
    "generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Example:\")\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"model: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated Example:    \n",
    "prompt: What strategies can be used to encourage users to provide more detailed and comprehensive answers during interviews?    \n",
    "model: What strategies can be used to encourage users to provide more detailed and comprehensive answers during interviews? the success in a startupbinator of your team for it you have, how idea on their product or company ideas that- are what growth's YC fundingIs from is some with business venture at goals when hard Com industry stage fit)? about early people new do starting startups were does this who after building time them as help together KP ( should first MVP factor up has an work they impact without into make market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }, \"/model.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
