{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "## 1. 개요\n",
        "\n",
        "- 구현 내용: 간단한 번역 모델을 만들어보며 시계열 데이터를 예측하는 순환신경망(Recurrent Neural Network) 코드를 구현합니다.\n",
        "- 코드 요약: 데이터를 로드한 뒤 전처리, 단어 임베딩, 인코더, 디코더를 구현합니다.\n",
        "- 참고 자료: 학교 수업 참고자료 (서울여자대학교)\n",
        "- 데이터 셋: [Tab-delimited Bilingual Sentence Pairs](https://www.manythings.org/anki/) 중 German - English 를 사용합니다. 코드 내에 데이터를 불러오는 과정을 포함합니다. [User-Agent는 여기](https://www.whatismybrowser.com/detect/what-is-my-user-agent/)를 참고하세요.\n",
        "- **주의 사항**: Tensorflow는 [tf.keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)가 지원 종료되기에 tf.keras.layers.TextVectorization 사용을 권장하고 있습니다."
      ],
      "metadata": {
        "id": "eczxV_3CWB4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 코드"
      ],
      "metadata": {
        "id": "wYS3nbnpWDYo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3blxCuk5iRw4"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import os, re, shutil, zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, LSTM, Masking\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 개요에 있는 User Agent 웹 사이트 참고하여 자신의 User-Agent 그대로 복사 붙여넣기\n",
        "headers = {\n",
        "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'\n",
        "}"
      ],
      "metadata": {
        "id": "jL2hdEHtjWOf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zipfile을 활용한 데이터 다운로드 함수\n",
        "def download_zip(url, output_path):\n",
        "  response = requests.get(url, headers=headers, stream=True)\n",
        "  if response.status_code == 200:\n",
        "    with open(output_path, 'wb') as f:\n",
        "      for chunk in response.iter_content(chunk_size=8192):\n",
        "        f.write(chunk)\n",
        "    print(f\"ZIP file downloaded to {output_path}\")\n",
        "  else:\n",
        "    print(f\"Failed to download. HTTP Response Code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "y718gvdxjy6x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# German - English 데이터 다운로드\n",
        "url = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
        "output_path = \"deu-eng.zip\"\n",
        "download_zip(url, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mv7wMzUkYrw",
        "outputId": "5ab06b05-12ae-460f-970c-324423c72152"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded to deu-eng.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로 조정 후 압축 해제\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, output_path)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "  zip_ref.extractall(path)"
      ],
      "metadata": {
        "id": "atmklp1pkfFU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 데이터 중 40000개 사용\n",
        "num_samples = 40000\n",
        "\n",
        "# 데이터 전처리 함수. (정규화 및 정제)\n",
        "def preprocess_sentence(sent):\n",
        "  # Nonspacing Mark (발음 기호 등) 제거\n",
        "  edited_sent = ''.join(c for c in unicodedata.normalize('NFD', sent) if unicodedata.category(c) != 'Mn')\n",
        "  # 단어와 구두점 사이에 공백 추가\n",
        "  edited_sent = re.sub(r\"([?.!,¿])\", r\" \\1\", edited_sent)\n",
        "  # 영문자, 문장 부호 외 전부 공백 변환\n",
        "  edited_sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", edited_sent)\n",
        "  # 공백이 여러 개일 때 하나로 축소\n",
        "  edited_sent = re.sub(r\"\\s+\", \" \", edited_sent)\n",
        "  return edited_sent"
      ],
      "metadata": {
        "id": "VA6T3MNCqo1A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 테스트\n",
        "en_sent = u\"Please select. Are the statements right or wrong?\"\n",
        "de_sent = u\"Wählen Sie. Sind die Aussagen Richtig oder Falsch?\"\n",
        "\n",
        "print('전처리 전 영어 문장 :', en_sent)\n",
        "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
        "print('전처리 전 독일어 문장 :', de_sent)\n",
        "print('전처리 후 독일어 문장 :', preprocess_sentence(de_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqjx5iHCwLJj",
        "outputId": "6ff2442c-fb71-45b6-bdaf-a35d01065ca0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리 전 영어 문장 : Please select. Are the statements right or wrong?\n",
            "전처리 후 영어 문장 : Please select . Are the statements right or wrong ?\n",
            "전처리 전 독일어 문장 : Wählen Sie. Sind die Aussagen Richtig oder Falsch?\n",
            "전처리 후 독일어 문장 : Wahlen Sie . Sind die Aussagen Richtig oder Falsch ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 불러오고 전처리 하는 함수\n",
        "def load_preprocessed_data(file_name):\n",
        "  encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "  with open(file_name, \"r\") as lines:\n",
        "    for i, line in enumerate(lines):\n",
        "      # source 데이터와 target 데이터 분리\n",
        "      # 분리된 문장 중 마지막은 라이선스 표기로 학습에 사용하지 않습니다.\n",
        "      src_line, tar_line, _ = line.strip().split('\\t')\n",
        "\n",
        "      # source 데이터 전처리\n",
        "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "      # target 데이터 전처리, sos, eos 표기 추가\n",
        "      tar_line = preprocess_sentence(tar_line)\n",
        "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "      encoder_input.append(src_line)\n",
        "      decoder_input.append(tar_line_in)\n",
        "      decoder_target.append(tar_line_out)\n",
        "\n",
        "      if i == num_samples - 1:\n",
        "        break\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "qfWStOXQwNhk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리된 데이터 확인\n",
        "sents_en_in, sents_deu_in, sents_deu_out = load_preprocessed_data(\"deu.txt\")\n",
        "print('인코더의 입력 :',sents_en_in[:3])\n",
        "print('디코더의 입력 :',sents_deu_in[:3])\n",
        "print('디코더의 레이블 :',sents_deu_out[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NDoaABodIgI",
        "outputId": "9ce70058-7347-4e4d-a7bb-9f3163307da0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력 : [['Go', '.'], ['Hi', '.'], ['Hi', '.']]\n",
            "디코더의 입력 : [['<sos>', 'Geh', '.'], ['<sos>', 'Hallo', '!'], ['<sos>', 'Gru', 'Gott', '!']]\n",
            "디코더의 레이블 : [['Geh', '.', '<eos>'], ['Hallo', '!', '<eos>'], ['Gru', 'Gott', '!', '<eos>']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 토크나이저 설정 (필터링 안함, 소문자 변환 안함)\n",
        "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
        "# 텍스트 목록을 기반으로 어휘 업데이트\n",
        "tokenizer_en.fit_on_texts(sents_en_in)\n",
        "# 텍스트 정수 시퀀스 변환\n",
        "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
        "# 자리가 남으면 뒤쪽에 padding을 추가 (기본값 0)\n",
        "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
        "\n",
        "# 독일어 토크나이저 설정 (같은 과정)\n",
        "tokenizer_deu = Tokenizer(filters=\"\", lower=False)\n",
        "tokenizer_deu.fit_on_texts(sents_deu_in)\n",
        "tokenizer_deu.fit_on_texts(sents_deu_out)\n",
        "\n",
        "decoder_input = tokenizer_deu.texts_to_sequences(sents_deu_in)\n",
        "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
        "\n",
        "decoder_target = tokenizer_deu.texts_to_sequences(sents_deu_out)\n",
        "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
      ],
      "metadata": {
        "id": "I5DS4ZDEwU6G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
        "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
        "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n",
        "\n",
        "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
        "tar_vocab_size = len(tokenizer_deu.word_index) + 1\n",
        "print(\"영어 단어 집합의 크기 : {:d}, 독일어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MUb67Tqwait",
        "outputId": "ebd3231e-3470-4bb5-b1b0-cd18ffe8e399"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더의 입력의 크기(shape) : (40000, 8)\n",
            "디코더의 입력의 크기(shape) : (40000, 12)\n",
            "디코더의 레이블의 크기(shape) : (40000, 12)\n",
            "영어 단어 집합의 크기 : 5761, 독일어 단어 집합의 크기 : 9518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어로부터 정수를 얻는 딕셔너리, 정수로부터 단어를 얻는 딕셔너리\n",
        "src_to_index = tokenizer_en.word_index\n",
        "index_to_src = tokenizer_en.index_word\n",
        "tar_to_index = tokenizer_deu.word_index\n",
        "index_to_tar = tokenizer_deu.index_word\n",
        "\n",
        "# 무작위로 순서 변경\n",
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print('무작위 시퀀스 :',indices)\n",
        "\n",
        "encoder_input = encoder_input[indices]\n",
        "decoder_input = decoder_input[indices]\n",
        "decoder_target = decoder_target[indices]\n",
        "\n",
        "print('encoder input 샘플 출력: ', encoder_input[30997])\n",
        "print('decoder input 샘플 출력: ',decoder_input[30997])\n",
        "print('decoder target 샘플 출력: ',decoder_target[30997])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlJfuZEKwdbu",
        "outputId": "fbfac6a7-033e-4cd6-e5e7-035997ea99a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "무작위 시퀀스 : [18391 27205 15572 ...  8877 26136 29447]\n",
            "encoder input 샘플 출력:  [128 894  31   0   0   0   0   0]\n",
            "decoder input 샘플 출력:  [   2  616   29   20 1035    9    0    0    0    0    0    0]\n",
            "decoder target 샘플 출력:  [ 616   29   20 1035    9    3    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터, 테스트 데이터 분리 (훈련 80/ 테스트 20)\n",
        "n_of_val = int(num_samples*0.2)\n",
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]\n",
        "\n",
        "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
        "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
        "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
        "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
        "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
        "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUwcdoMHwgys",
        "outputId": "fa99f2df-bb00-46e4-87a8-f2d7c3a00a1a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 source 데이터의 크기 : (32000, 8)\n",
            "훈련 target 데이터의 크기 : (32000, 12)\n",
            "훈련 target 레이블의 크기 : (32000, 12)\n",
            "테스트 source 데이터의 크기 : (8000, 8)\n",
            "테스트 target 데이터의 크기 : (8000, 12)\n",
            "테스트 target 레이블의 크기 : (8000, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 차원: 하나의 토큰을 몇 개의 벡터로서 표현할 것인가\n",
        "embedding_dim = 64\n",
        "hidden_units = 64"
      ],
      "metadata": {
        "id": "cVq-L6lyxRhb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(src_vocab_size, embedding_dim)(encoder_inputs)\n",
        "# 연산에서 0(패딩 값) 제외\n",
        "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
        "# 가변 길이 input과 output, 내부 상태 반환을 위해 return_state = True\n",
        "encoder_lstm = LSTM(hidden_units, return_state=True)\n",
        "# 은닉 상태와 셀 상태를 저장\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "JqDnX7srxTUs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "# 재사용하기 위해 레이어 저장\n",
        "dec_emb_layer = Embedding(tar_vocab_size, hidden_units)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
        "# 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "# 인코더의 은닉 상태를 초기 은닉 상태로 사용\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
        "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
        "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "O6guz8AUxVLS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
        "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
        "          batch_size=128, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7bOR4CtxXwp",
        "outputId": "2b358ed4-674b-4c34-cc55-b8c4ea5b5d1a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 138s 528ms/step - loss: 3.8489 - acc: 0.5091 - val_loss: 2.5580 - val_acc: 0.5437\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 128s 512ms/step - loss: 2.3516 - acc: 0.6138 - val_loss: 2.2148 - val_acc: 0.6844\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 129s 516ms/step - loss: 2.1015 - acc: 0.6852 - val_loss: 2.0521 - val_acc: 0.6881\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 128s 513ms/step - loss: 1.9568 - acc: 0.6904 - val_loss: 1.9153 - val_acc: 0.6996\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 130s 520ms/step - loss: 1.8184 - acc: 0.7039 - val_loss: 1.7990 - val_acc: 0.7094\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 129s 516ms/step - loss: 1.6982 - acc: 0.7232 - val_loss: 1.7026 - val_acc: 0.7390\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 129s 517ms/step - loss: 1.5958 - acc: 0.7465 - val_loss: 1.6177 - val_acc: 0.7511\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 129s 517ms/step - loss: 1.5187 - acc: 0.7574 - val_loss: 1.5627 - val_acc: 0.7608\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 160s 639ms/step - loss: 1.4558 - acc: 0.7649 - val_loss: 1.5141 - val_acc: 0.7662\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 139s 558ms/step - loss: 1.4001 - acc: 0.7708 - val_loss: 1.4738 - val_acc: 0.7713\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 150s 601ms/step - loss: 1.3491 - acc: 0.7774 - val_loss: 1.4388 - val_acc: 0.7758\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 131s 522ms/step - loss: 1.2999 - acc: 0.7834 - val_loss: 1.4058 - val_acc: 0.7800\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 130s 520ms/step - loss: 1.2526 - acc: 0.7895 - val_loss: 1.3724 - val_acc: 0.7852\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 157s 630ms/step - loss: 1.2067 - acc: 0.7954 - val_loss: 1.3429 - val_acc: 0.7884\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 138s 551ms/step - loss: 1.1631 - acc: 0.8003 - val_loss: 1.3158 - val_acc: 0.7918\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 157s 630ms/step - loss: 1.1225 - acc: 0.8050 - val_loss: 1.2917 - val_acc: 0.7957\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 156s 626ms/step - loss: 1.0845 - acc: 0.8090 - val_loss: 1.2686 - val_acc: 0.7979\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 158s 631ms/step - loss: 1.0485 - acc: 0.8126 - val_loss: 1.2507 - val_acc: 0.8004\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 159s 636ms/step - loss: 1.0157 - acc: 0.8162 - val_loss: 1.2341 - val_acc: 0.8021\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 139s 557ms/step - loss: 0.9846 - acc: 0.8196 - val_loss: 1.2178 - val_acc: 0.8043\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 139s 554ms/step - loss: 0.9547 - acc: 0.8231 - val_loss: 1.2049 - val_acc: 0.8062\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 158s 634ms/step - loss: 0.9263 - acc: 0.8262 - val_loss: 1.1918 - val_acc: 0.8082\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 157s 629ms/step - loss: 0.8991 - acc: 0.8295 - val_loss: 1.1779 - val_acc: 0.8089\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 157s 630ms/step - loss: 0.8724 - acc: 0.8326 - val_loss: 1.1676 - val_acc: 0.8106\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 161s 646ms/step - loss: 0.8470 - acc: 0.8360 - val_loss: 1.1581 - val_acc: 0.8114\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 160s 641ms/step - loss: 0.8224 - acc: 0.8391 - val_loss: 1.1490 - val_acc: 0.8134\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 158s 635ms/step - loss: 0.7984 - acc: 0.8422 - val_loss: 1.1386 - val_acc: 0.8161\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 140s 562ms/step - loss: 0.7757 - acc: 0.8454 - val_loss: 1.1319 - val_acc: 0.8164\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 138s 551ms/step - loss: 0.7534 - acc: 0.8483 - val_loss: 1.1250 - val_acc: 0.8178\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 138s 552ms/step - loss: 0.7314 - acc: 0.8515 - val_loss: 1.1187 - val_acc: 0.8195\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 139s 556ms/step - loss: 0.7097 - acc: 0.8546 - val_loss: 1.1104 - val_acc: 0.8205\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 159s 635ms/step - loss: 0.6891 - acc: 0.8575 - val_loss: 1.1045 - val_acc: 0.8210\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 161s 644ms/step - loss: 0.6694 - acc: 0.8605 - val_loss: 1.0987 - val_acc: 0.8226\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 159s 637ms/step - loss: 0.6494 - acc: 0.8637 - val_loss: 1.0930 - val_acc: 0.8232\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 142s 570ms/step - loss: 0.6304 - acc: 0.8667 - val_loss: 1.0862 - val_acc: 0.8253\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 160s 642ms/step - loss: 0.6117 - acc: 0.8704 - val_loss: 1.0831 - val_acc: 0.8261\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 160s 640ms/step - loss: 0.5935 - acc: 0.8732 - val_loss: 1.0776 - val_acc: 0.8272\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 161s 646ms/step - loss: 0.5762 - acc: 0.8764 - val_loss: 1.0718 - val_acc: 0.8283\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 161s 644ms/step - loss: 0.5584 - acc: 0.8799 - val_loss: 1.0692 - val_acc: 0.8294\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 143s 571ms/step - loss: 0.5416 - acc: 0.8827 - val_loss: 1.0672 - val_acc: 0.8302\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 161s 645ms/step - loss: 0.5253 - acc: 0.8859 - val_loss: 1.0597 - val_acc: 0.8318\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 162s 647ms/step - loss: 0.5091 - acc: 0.8891 - val_loss: 1.0572 - val_acc: 0.8324\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 140s 560ms/step - loss: 0.4938 - acc: 0.8920 - val_loss: 1.0542 - val_acc: 0.8338\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 161s 645ms/step - loss: 0.4791 - acc: 0.8947 - val_loss: 1.0519 - val_acc: 0.8345\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 160s 642ms/step - loss: 0.4648 - acc: 0.8976 - val_loss: 1.0486 - val_acc: 0.8355\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 160s 642ms/step - loss: 0.4510 - acc: 0.9005 - val_loss: 1.0480 - val_acc: 0.8361\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 140s 560ms/step - loss: 0.4378 - acc: 0.9028 - val_loss: 1.0469 - val_acc: 0.8360\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 138s 552ms/step - loss: 0.4248 - acc: 0.9052 - val_loss: 1.0457 - val_acc: 0.8372\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 138s 553ms/step - loss: 0.4124 - acc: 0.9081 - val_loss: 1.0442 - val_acc: 0.8376\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 157s 629ms/step - loss: 0.4008 - acc: 0.9104 - val_loss: 1.0415 - val_acc: 0.8383\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c9751177ac0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역할 문장 --(인코더)--> 마지막 시점 은닉 상태, 셀 상태, <sos> --(디코더)--> 예측값 (eos까지 반복하여 생성)\n",
        "\n",
        "# 학습 후 인코더\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# 디코더에 이전 시점의 상태를 보관\n",
        "decoder_state_input_h = Input(shape=(hidden_units,))\n",
        "decoder_state_input_c = Input(shape=(hidden_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 훈련 때 사용했던 임베딩 층 재사용\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# 이전 시점의 상태를 현 시점의 초기 상태로 사용하여 다음 단어 예측\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]"
      ],
      "metadata": {
        "id": "W1x59ScfxaqM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 시점에 대해서 단어 예측\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)"
      ],
      "metadata": {
        "id": "QnaM2jfOxfQw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 후 디코더\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "# 디코딩 함수\n",
        "def decode_sequence(input_seq):\n",
        "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "  # <sos>에 해당하는 정수 생성\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0, 0] = tar_to_index['<sos>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  # stop_condition이 True가 될 때까지 루프 반복\n",
        "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정\n",
        "  while not stop_condition:\n",
        "    # 이전 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    # 예측 결과를 단어로 변환\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
        "    decoded_sentence += ' '+sampled_char\n",
        "\n",
        "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단\n",
        "    if (sampled_char == '<eos>' or\n",
        "        len(decoded_sentence) > 50):\n",
        "        stop_condition = True\n",
        "\n",
        "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "    states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "Rp_KjWotx2iR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 원래 문장의 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수\n",
        "def seq_to_src(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0):\n",
        "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
        "  return sentence\n",
        "\n",
        "# 번역한 문장의 정수 시퀀스를 텍스트 시퀀스로 변환하는 함수\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence = ''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
        "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "hVjaIJ5bzbYh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역 테스트\n",
        "for seq_index in [3, 1020, 3204, 6025]:\n",
        "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuGIS1pkzds8",
        "outputId": "3141d640-9423-4627-c08e-b08a0929e1b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 445ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "입력문장 : Get a move on ! \n",
            "정답문장 : Gib mal Gas ! \n",
            "번역문장 : Hol mal eine Minute . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "입력문장 : Tom was wet . \n",
            "정답문장 : Tom war nass . \n",
            "번역문장 : Tom war schuchtern . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "입력문장 : I can t see anyone . \n",
            "정답문장 : Ich kann niemanden sehen . \n",
            "번역문장 : Ich kann nichts artig . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "입력문장 : I m ruined . \n",
            "정답문장 : Ich bin ruiniert . \n",
            "번역문장 : Ich bin ruiniert . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역 테스트\n",
        "for seq_index in [3, 50, 1020, 3204, 6025]:\n",
        "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
        "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
        "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "pzeznsAYSP_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fb31a6-f98c-4a06-f4aa-b78294e10271"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "입력문장 : I ve got an idea . \n",
            "정답문장 : Ich habe eine Idee . \n",
            "번역문장 : Ich habe eine gute Arbeit . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "입력문장 : Close the door . \n",
            "정답문장 : Mach die Ture zu . \n",
            "번역문장 : Schlie die Tur . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "입력문장 : I get your drift . \n",
            "정답문장 : Ich verstehe was du sagen willst . \n",
            "번역문장 : Ich verstehe was ihr sagen . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : I won t stop you . \n",
            "정답문장 : Ich werde dir nichts in den Weg legen . \n",
            "번역문장 : Ich werde dich nicht anlugen . \n",
            "--------------------------------------------------\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "입력문장 : I am short of money . \n",
            "정답문장 : Mir geht das Geld aus . \n",
            "번역문장 : Ich bin knapp bei Kasse . \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}